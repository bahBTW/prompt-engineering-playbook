# Prompt Evaluation Framework

Prompts are evaluated using the following criteria:

- **Correctness**  
  Does the output solve the task correctly?

- **Format Adherence**  
  Is the output strictly following the requested format?

- **Consistency**  
  Does the prompt produce stable results across multiple runs?

- **Hallucination Risk**  
  Does the model introduce unsupported or fabricated information?

- **Token Efficiency**  
  Is the prompt concise while maintaining quality?

This framework allows systematic comparison
between prompt iterations.
